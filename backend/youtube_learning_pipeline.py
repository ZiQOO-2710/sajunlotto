#!/usr/bin/env python3
"""
YouTube ì‚¬ì£¼ ì½˜í…ì¸  ìë™ í•™ìŠµ íŒŒì´í”„ë¼ì¸
í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ AI ì§€ì‹ë² ì´ìŠ¤ì— í†µí•©í•˜ê³  ì§€ì†ì ìœ¼ë¡œ í•™ìŠµ
"""

import sqlite3
import json
import asyncio
from typing import List, Dict, Any
from datetime import datetime
import numpy as np
from app.services.youtube_service import YouTubeService

class SajuLearningPipeline:
    """ì‚¬ì£¼ í•™ìŠµ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.crawled_db = 'saju_youtube_data.db'
        self.knowledge_db = 'saju_knowledge_complete.db'
        
        # í•™ìŠµ í†µê³„
        self.stats = {
            'total_processed': 0,
            'successful_integrations': 0,
            'failed_integrations': 0,
            'average_confidence': 0
        }
    
    def load_structured_data(self) -> List[Dict]:
        """í¬ë¡¤ë§ëœ êµ¬ì¡°í™” ë°ì´í„° ë¡œë“œ"""
        conn = sqlite3.connect(self.crawled_db)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT 
                sld.*, 
                cv.title as video_title, 
                cv.channel_name,
                cv.view_count,
                cv.subscriber_count
            FROM structured_learning_data sld
            JOIN crawled_videos cv ON sld.video_id = cv.video_id
            WHERE sld.confidence_score > 0.6
            ORDER BY cv.view_count DESC
        """)
        
        data = []
        for row in cursor.fetchall():
            data.append({
                'id': row[0],
                'video_id': row[1],
                'question': row[2],
                'answer': row[3],
                'saju_info': row[4],
                'interpretation': row[5],
                'confidence_score': row[6],
                'video_title': row[8],
                'channel_name': row[9],
                'view_count': row[10],
                'subscriber_count': row[11]
            })
        
        conn.close()
        return data
    
    def enhance_with_saju_terms(self, text: str) -> Dict[str, List[str]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ì£¼ ì „ë¬¸ìš©ì–´ ì¶”ì¶œ ë° ê°•í™”"""
        found_terms = {}
        
        # ì²œê°„ ë§¤ì¹­
        ì²œê°„ = ['ê°‘', 'ì„', 'ë³‘', 'ì •', 'ë¬´', 'ê¸°', 'ê²½', 'ì‹ ', 'ì„', 'ê³„']
        found_terms['ì²œê°„'] = [term for term in ì²œê°„ if term in text]
        
        # ì§€ì§€ ë§¤ì¹­
        ì§€ì§€ = ['ì', 'ì¶•', 'ì¸', 'ë¬˜', 'ì§„', 'ì‚¬', 'ì˜¤', 'ë¯¸', 'ì‹ ', 'ìœ ', 'ìˆ ', 'í•´']
        found_terms['ì§€ì§€'] = [term for term in ì§€ì§€ if term in text]
        
        # ì˜¤í–‰ ë§¤ì¹­
        ì˜¤í–‰ = ['ëª©', 'í™”', 'í† ', 'ê¸ˆ', 'ìˆ˜']
        found_terms['ì˜¤í–‰'] = [term for term in ì˜¤í–‰ if term in text]
        
        # ì‹­ì‹  ë§¤ì¹­
        ì‹­ì‹  = ['ë¹„ê²¬', 'ê²ì¬', 'ì‹ì‹ ', 'ìƒê´€', 'í¸ì¬', 'ì •ì¬', 'í¸ê´€', 'ì •ê´€', 'í¸ì¸', 'ì •ì¸']
        found_terms['ì‹­ì‹ '] = [term for term in ì‹­ì‹  if term in text]
        
        # ì¼ì£¼ íŒ¨í„´ ë§¤ì¹­
        import re
        ì¼ì£¼_pattern = r'([ê°‘ì„ë³‘ì •ë¬´ê¸°ê²½ì‹ ì„ê³„][ëª©í™”í† ê¸ˆìˆ˜])\s*ì¼ì£¼'
        ì¼ì£¼_matches = re.findall(ì¼ì£¼_pattern, text)
        if ì¼ì£¼_matches:
            found_terms['ì¼ì£¼'] = ì¼ì£¼_matches
        
        return found_terms
    
    def calculate_enhanced_confidence(self, data: Dict) -> float:
        """í–¥ìƒëœ ì‹ ë¢°ë„ ê³„ì‚°"""
        base_confidence = data.get('confidence_score', 0.5)
        
        # ì¡°íšŒìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜
        view_weight = min(data['view_count'] / 1000000, 0.2)  # ìµœëŒ€ 0.2
        
        # êµ¬ë…ì ê¸°ë°˜ ê°€ì¤‘ì¹˜
        subscriber_weight = min(data['subscriber_count'] / 100000, 0.1)  # ìµœëŒ€ 0.1
        
        # ë‹µë³€ ê¸¸ì´ ê¸°ë°˜ ê°€ì¤‘ì¹˜
        answer_length = len(data.get('answer', ''))
        length_weight = min(answer_length / 500, 0.1)  # ìµœëŒ€ 0.1
        
        # ì „ë¬¸ìš©ì–´ í¬í•¨ ê°€ì¤‘ì¹˜
        terms = self.enhance_with_saju_terms(data.get('answer', ''))
        term_count = sum(len(v) for v in terms.values())
        term_weight = min(term_count * 0.02, 0.1)  # ìµœëŒ€ 0.1
        
        # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
        enhanced_confidence = base_confidence + view_weight + subscriber_weight + length_weight + term_weight
        
        return min(enhanced_confidence, 1.0)
    
    def integrate_to_knowledge_base(self, data: Dict) -> bool:
        """ì§€ì‹ë² ì´ìŠ¤ì— í†µí•©"""
        try:
            conn = sqlite3.connect(self.knowledge_db)
            cursor = conn.cursor()
            
            # ì‚¬ì£¼ ì „ë¬¸ìš©ì–´ ì¶”ì¶œ
            saju_terms = self.enhance_with_saju_terms(data['answer'])
            
            # ë¬¸ì¥ ìœ í˜• ë¶„ë¥˜
            sentence_type = self.classify_sentence_type(data['question'], data['answer'])
            
            # í–¥ìƒëœ ì‹ ë¢°ë„ ê³„ì‚°
            enhanced_confidence = self.calculate_enhanced_confidence(data)
            
            # ì§€ì‹ë² ì´ìŠ¤ì— ì‚½ì…
            cursor.execute("""
                INSERT INTO saju_knowledge 
                (video_id, video_title, content, saju_terms, 
                 sentence_type, confidence, source)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                data['video_id'],
                data['video_title'],
                json.dumps({
                    'question': data['question'],
                    'answer': data['answer']
                }),
                json.dumps(saju_terms),
                sentence_type,
                enhanced_confidence,
                'youtube_crawler'
            ))
            
            conn.commit()
            conn.close()
            return True
            
        except Exception as e:
            print(f"âŒ í†µí•© ì‹¤íŒ¨: {e}")
            return False
    
    def classify_sentence_type(self, question: str, answer: str) -> str:
        """ë¬¸ì¥ ìœ í˜• ë¶„ë¥˜"""
        combined_text = f"{question} {answer}".lower()
        
        if any(word in combined_text for word in ['ì„±ê²©', 'ê¸°ì§ˆ', 'ì„±í–¥', 'íŠ¹ì„±']):
            return 'personality'
        elif any(word in combined_text for word in ['ìš´ì„¸', 'ë¯¸ë˜', 'ì˜ˆì¸¡', 'ì•ìœ¼ë¡œ']):
            return 'prediction'
        elif any(word in combined_text for word in ['ê´€ê³„', 'ê¶í•©', 'ë§Œë‚¨', 'ì—°ì• ']):
            return 'relationship'
        elif any(word in combined_text for word in ['ê±´ê°•', 'ì²´ì§ˆ', 'ì§ˆë³‘']):
            return 'health'
        elif any(word in combined_text for word in ['ì¬ë¬¼', 'ëˆ', 'ì‚¬ì—…', 'íˆ¬ì']):
            return 'wealth'
        elif any(word in combined_text for word in ['í•´ì„', 'ì˜ë¯¸', 'ëœ»']):
            return 'interpretation'
        else:
            return 'general'
    
    async def run_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ ì‚¬ì£¼ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
        
        # 1. êµ¬ì¡°í™”ëœ ë°ì´í„° ë¡œë“œ
        structured_data = self.load_structured_data()
        print(f"ğŸ“Š ë¡œë“œëœ ë°ì´í„°: {len(structured_data)}ê°œ")
        
        # 2. ê° ë°ì´í„°ë¥¼ ì§€ì‹ë² ì´ìŠ¤ì— í†µí•©
        for data in structured_data:
            success = self.integrate_to_knowledge_base(data)
            
            if success:
                self.stats['successful_integrations'] += 1
            else:
                self.stats['failed_integrations'] += 1
            
            self.stats['total_processed'] += 1
            
            # ì§„í–‰ìƒí™© í‘œì‹œ
            if self.stats['total_processed'] % 10 == 0:
                print(f"  ì²˜ë¦¬ ì¤‘... {self.stats['total_processed']}/{len(structured_data)}")
        
        # 3. í†µê³„ ì¶œë ¥
        self.print_statistics()
    
    def print_statistics(self):
        """í•™ìŠµ í†µê³„ ì¶œë ¥"""
        print(f"""
        ========================================
        ğŸ“ˆ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ
        ========================================
        ì´ ì²˜ë¦¬: {self.stats['total_processed']}ê°œ
        ì„±ê³µ: {self.stats['successful_integrations']}ê°œ
        ì‹¤íŒ¨: {self.stats['failed_integrations']}ê°œ
        ì„±ê³µë¥ : {self.stats['successful_integrations']/max(self.stats['total_processed'], 1)*100:.1f}%
        ========================================
        """)
    
    def query_learned_knowledge(self, query: str, limit: int = 5):
        """í•™ìŠµëœ ì§€ì‹ ê²€ìƒ‰"""
        conn = sqlite3.connect(self.knowledge_db)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT content, saju_terms, confidence, video_title
            FROM saju_knowledge
            WHERE content LIKE ?
            ORDER BY confidence DESC
            LIMIT ?
        """, (f"%{query}%", limit))
        
        results = cursor.fetchall()
        conn.close()
        
        print(f"\nğŸ” '{query}' ê²€ìƒ‰ ê²°ê³¼:")
        for i, (content, terms, confidence, title) in enumerate(results, 1):
            content_data = json.loads(content)
            print(f"""
            {i}. [{confidence:.2f}] {title[:50]}...
               Q: {content_data.get('question', '')[:100]}
               A: {content_data.get('answer', '')[:200]}...
            """)

class AutomaticLearningScheduler:
    """ìë™ í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self):
        self.pipeline = SajuLearningPipeline()
        self.crawling_interval = 86400  # 24ì‹œê°„ë§ˆë‹¤ í¬ë¡¤ë§
        self.learning_interval = 3600    # 1ì‹œê°„ë§ˆë‹¤ í•™ìŠµ
    
    async def continuous_learning(self):
        """ì§€ì†ì  í•™ìŠµ ë£¨í”„"""
        while True:
            try:
                # í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                await self.pipeline.run_pipeline()
                
                # ëŒ€ê¸°
                await asyncio.sleep(self.learning_interval)
                
            except Exception as e:
                print(f"âŒ í•™ìŠµ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(60)  # ì˜¤ë¥˜ ì‹œ 1ë¶„ ëŒ€ê¸°

async def main():
    # í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = SajuLearningPipeline()
    await pipeline.run_pipeline()
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    pipeline.query_learned_knowledge("ê°‘ëª© ì¼ì£¼")
    pipeline.query_learned_knowledge("ì¬ë¬¼ìš´")
    pipeline.query_learned_knowledge("ì—°ì• ìš´")

if __name__ == "__main__":
    asyncio.run(main())