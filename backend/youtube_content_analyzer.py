#!/usr/bin/env python3
"""
ìœ íŠœë¸Œ ì˜ìƒ ì „ì²´ ì½˜í…ì¸  ë¶„ì„ ë° ì‚¬ì£¼ ì§€ì‹ í•™ìŠµ ì‹œìŠ¤í…œ
- ìë§‰ ì¶”ì¶œ ë° ë¶„ì„
- ì˜ìƒ ë‹¤ìš´ë¡œë“œ ë° ìŒì„± ì¶”ì¶œ
- STT(Speech-to-Text) ë³€í™˜
- í†µí•© ì½˜í…ì¸  ë¶„ì„ ë° ì§€ì‹ í•™ìŠµ
"""

import os
import re
import json
import tempfile
import shutil
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import sqlite3

# YouTube ê´€ë ¨
import yt_dlp
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter

# ìŒì„± ì²˜ë¦¬ (ì„ íƒì )
try:
    import whisper
    import torch
    from moviepy.editor import VideoFileClip
    from pydub import AudioSegment
    AUDIO_PROCESSING_AVAILABLE = True
except ImportError as e:
    print(f"ìŒì„± ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½ (ìë§‰ ê¸°ë°˜ í•™ìŠµì€ ê°€ëŠ¥): {e}")
    AUDIO_PROCESSING_AVAILABLE = False

# AI/NLP
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
except ImportError:
    print("sentence-transformers ì„¤ì¹˜ í•„ìš”: pip install sentence-transformers")

from youtube_crawler import YouTubeSajuCrawler

class YouTubeContentAnalyzer:
    """ìœ íŠœë¸Œ ì˜ìƒ ì „ì²´ ì½˜í…ì¸  ë¶„ì„ ë° í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self, knowledge_db_path: str = "saju_knowledge_complete.db", temp_dir: str = None):
        """
        ì´ˆê¸°í™”
        Args:
            knowledge_db_path: ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
            temp_dir: ì„ì‹œ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.knowledge_db_path = knowledge_db_path
        self.temp_dir = temp_dir or tempfile.mkdtemp()
        Path(self.temp_dir).mkdir(exist_ok=True)
        
        self.setup_knowledge_database()
        self.setup_ai_models()
        
        # ì‚¬ì£¼ ê´€ë ¨ í•µì‹¬ ìš©ì–´ë“¤ (í™•ì¥)
        self.saju_terms = {
            'ì²œê°„': ['ê°‘', 'ì„', 'ë³‘', 'ì •', 'ë¬´', 'ê¸°', 'ê²½', 'ì‹ ', 'ì„', 'ê³„'],
            'ì§€ì§€': ['ì', 'ì¶•', 'ì¸', 'ë¬˜', 'ì§„', 'ì‚¬', 'ì˜¤', 'ë¯¸', 'ì‹ ', 'ìœ ', 'ìˆ ', 'í•´'],
            'ì˜¤í–‰': ['ëª©', 'í™”', 'í† ', 'ê¸ˆ', 'ìˆ˜', 'ëª©í–‰', 'í™”í–‰', 'í† í–‰', 'ê¸ˆí–‰', 'ìˆ˜í–‰'],
            'ìŒì–‘': ['ì–‘', 'ìŒ', 'ì–‘ê°„', 'ìŒê°„', 'ì–‘ì§€', 'ìŒì§€'],
            'ì‹­ì‹ ': ['ë¹„ê²¬', 'ê²ì¬', 'ì‹ì‹ ', 'ìƒê´€', 'í¸ì¬', 'ì •ì¬', 'í¸ê´€', 'ì •ê´€', 'í¸ì¸', 'ì •ì¸'],
            'ìš´ì„¸_ê°œë…': ['ëŒ€ìš´', 'ì„¸ìš´', 'ì›”ìš´', 'ì¼ìš´', 'ì‹ ì‚´', 'ê³µë§', 'í˜•ì¶©íŒŒí•´', 'ì›ì§„', 'ìœ¡í•©', 'ì‚¼í•©'],
            'ì‚¬ì£¼_êµ¬ì„±': ['ë…„ì£¼', 'ì›”ì£¼', 'ì¼ì£¼', 'ì‹œì£¼', 'ì¼ê°„', 'ì›”ë ¹', 'ë…„ê°„', 'ì›”ê°„', 'ì‹œê°„'],
            'ë ë³„': ['ì¥ë ', 'ì†Œë ', 'í˜¸ë‘ì´ë ', 'í† ë¼ë ', 'ìš©ë ', 'ë±€ë ', 'ë§ë ', 'ì–‘ë ', 'ì›ìˆ­ì´ë ', 'ë‹­ë ', 'ê°œë ', 'ë¼ì§€ë '],
            'ìš´ì„¸_ì˜ì—­': ['ì¬ë¬¼ìš´', 'ê±´ê°•ìš´', 'ì—°ì• ìš´', 'ê²°í˜¼ìš´', 'ì§ì¥ìš´', 'í•™ì—…ìš´', 'ì¸ê°„ê´€ê³„'],
            'ì‚¬ì£¼_í•´ì„': ['ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'ê¸¸í•˜ë‹¤', 'í‰í•˜ë‹¤', 'ë°œì „', 'ì‡ í‡´', 'ì„±ê³µ', 'ì‹¤íŒ¨', 'ì¡°ì‹¬', 'ì£¼ì˜']
        }
        
        # yt-dlp ì„¤ì • (ìŒì„±ë§Œ ì¶”ì¶œ)
        self.ytdl_opts = {
            'format': 'bestaudio/best',
            'outtmpl': os.path.join(self.temp_dir, '%(id)s.%(ext)s'),
            'quiet': True,
            'no_warnings': True,
        }
    
    def setup_ai_models(self):
        """AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        print("ğŸ¤– AI ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # Whisper STT ëª¨ë¸ (í•œêµ­ì–´ ì§€ì›) - ì„ íƒì 
        if AUDIO_PROCESSING_AVAILABLE:
            try:
                self.whisper_model = whisper.load_model("base")  # base, small, medium, large ì„ íƒ ê°€ëŠ¥
                print("âœ… Whisper STT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ Whisper ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ìë§‰ ê¸°ë°˜ í•™ìŠµë§Œ ì‚¬ìš©): {e}")
                self.whisper_model = None
        else:
            print("â„¹ï¸ ìŒì„± ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ìë§‰ ê¸°ë°˜ í•™ìŠµë§Œ ì‚¬ìš©")
            self.whisper_model = None
        
        # í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸
        try:
            self.embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
            print("âœ… í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ê¸°ë³¸ ë¶„ì„ë§Œ ì‚¬ìš©): {e}")
            self.embedding_model = None
    
    def setup_knowledge_database(self):
        """í™•ì¥ëœ ì§€ì‹ ë² ì´ìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.knowledge_db_path)
        cursor = conn.cursor()
        
        # ì˜ìƒ ë©”íƒ€ë°ì´í„°
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS video_content (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                video_id TEXT UNIQUE NOT NULL,
                title TEXT,
                channel_title TEXT,
                duration_seconds INTEGER,
                view_count INTEGER,
                like_count INTEGER,
                
                -- ìë§‰ ê´€ë ¨
                has_transcript BOOLEAN DEFAULT FALSE,
                transcript_text TEXT,
                transcript_language TEXT,
                
                -- ìŒì„± ê´€ë ¨  
                has_audio_extracted BOOLEAN DEFAULT FALSE,
                audio_transcription TEXT,
                audio_quality_score REAL,
                
                -- í†µí•© ë¶„ì„
                combined_text TEXT, -- ìë§‰ + STT í†µí•©
                total_word_count INTEGER,
                saju_relevance_score REAL,
                content_quality_score REAL,
                
                processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                file_size_mb REAL
            )
        ''')
        
        # ì§€ì‹ ë©ì–´ë¦¬ë“¤ (ë¬¸ì¥/ë¬¸ë‹¨ ë‹¨ìœ„)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS knowledge_segments (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                video_id TEXT,
                segment_text TEXT,
                segment_type TEXT, -- transcript, audio_stt, combined
                time_start REAL, -- ì˜ìƒ ë‚´ ì‹œì‘ ì‹œê°„ (ì´ˆ)
                time_end REAL,   -- ì˜ìƒ ë‚´ ë ì‹œê°„ (ì´ˆ)
                
                -- ì‚¬ì£¼ ë¶„ì„
                saju_terms TEXT, -- JSON array
                knowledge_category TEXT, -- ì˜ˆì¸¡, í•´ì„, ì´ë¡ , ì‹¤ì „ ë“±
                confidence_score REAL,
                
                -- ì„ë² ë”©
                embedding BLOB,
                
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (video_id) REFERENCES video_content (video_id)
            )
        ''')
        
        # í•™ìŠµëœ ì‚¬ì£¼ ì§€ì‹ íŒ¨í„´
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS saju_knowledge_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern_type TEXT, -- í•´ì„íŒ¨í„´, ì˜ˆì¸¡íŒ¨í„´, ê¶í•©íŒ¨í„´ ë“±
                condition_elements TEXT, -- JSON: ì¡°ê±´ ì‚¬ì£¼ ìš”ì†Œ
                result_interpretation TEXT, -- ê²°ê³¼ í•´ì„
                confidence_level REAL,
                source_videos TEXT, -- JSON: ì¶œì²˜ ì˜ìƒë“¤
                example_sentences TEXT, -- JSON: ì‹¤ì œ ì˜ˆì‹œ ë¬¸ì¥ë“¤
                frequency_count INTEGER DEFAULT 1,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ì‚¬ì£¼ ìš©ì–´ ì‚¬ì „ (í™•ì¥)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS saju_dictionary (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                term TEXT UNIQUE NOT NULL,
                category TEXT,
                meaning TEXT,
                detailed_explanation TEXT,
                usage_examples TEXT, -- JSON array
                related_terms TEXT, -- JSON array
                frequency INTEGER DEFAULT 0,
                confidence REAL DEFAULT 1.0,
                sources TEXT, -- JSON: ì¶œì²˜ ì˜ìƒë“¤
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ì‹¤ì‹œê°„ í•™ìŠµ ë¡œê·¸
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learning_progress (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                video_id TEXT,
                processing_stage TEXT, -- download, transcript, stt, analysis, learning
                status TEXT, -- success, failed, processing
                details TEXT, -- JSON with detailed info
                processing_time_seconds REAL,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        print("ğŸ“Š í™•ì¥ëœ ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def download_audio(self, video_id: str) -> Optional[str]:
        """
        YouTube ì˜ìƒì—ì„œ ìŒì„±ë§Œ ì¶”ì¶œí•´ì„œ ë‹¤ìš´ë¡œë“œ
        
        Args:
            video_id: YouTube ì˜ìƒ ID
            
        Returns:
            ë‹¤ìš´ë¡œë“œëœ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
        """
        try:
            self._log_progress(video_id, 'download', 'processing', {'message': 'ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì‹œì‘'})
            
            url = f"https://www.youtube.com/watch?v={video_id}"
            
            with yt_dlp.YoutubeDL(self.ytdl_opts) as ydl:
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                info = ydl.extract_info(url, download=False)
                duration = info.get('duration', 0)
                
                # ë„ˆë¬´ ê¸´ ì˜ìƒì€ ê±´ë„ˆë›°ê¸° (30ë¶„ ì´ìƒ)
                if duration > 1800:
                    self._log_progress(video_id, 'download', 'failed', 
                                     {'message': f'ì˜ìƒì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {duration/60:.1f}ë¶„'})
                    return None
                
                # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ
                ydl.download([url])
                
                # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì°¾ê¸°
                for file in os.listdir(self.temp_dir):
                    if video_id in file and (file.endswith('.m4a') or file.endswith('.webm') or file.endswith('.mp3')):
                        audio_path = os.path.join(self.temp_dir, file)
                        file_size = os.path.getsize(audio_path) / (1024 * 1024)  # MB
                        
                        self._log_progress(video_id, 'download', 'success', 
                                         {'duration': duration, 'file_size_mb': file_size})
                        return audio_path
                
                return None
                
        except Exception as e:
            self._log_progress(video_id, 'download', 'failed', {'error': str(e)})
            print(f"âŒ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({video_id}): {str(e)}")
            return None
    
    def extract_transcript(self, video_id: str) -> Optional[Dict]:
        """ìë§‰ ì¶”ì¶œ (ê¸°ì¡´ ë°©ì‹ ê°œì„ )"""
        try:
            self._log_progress(video_id, 'transcript', 'processing', {'message': 'ìë§‰ ì¶”ì¶œ ì‹œì‘'})
            
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # í•œêµ­ì–´ ìë§‰ ìš°ì„ 
            transcript = None
            language_used = None
            
            try:
                transcript = transcript_list.find_transcript(['ko'])
                language_used = 'ko'
            except:
                try:
                    transcript = transcript_list.find_generated_transcript(['ko'])
                    language_used = 'ko-auto'
                except:
                    try:
                        transcript = transcript_list.find_transcript(['en'])
                        language_used = 'en'
                    except:
                        self._log_progress(video_id, 'transcript', 'failed', {'message': 'ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'})
                        return None
            
            # ì‹œê°„ ì •ë³´ì™€ í•¨ê»˜ ìë§‰ ì¶”ì¶œ
            transcript_data = transcript.fetch()
            
            # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            formatter = TextFormatter()
            full_text = formatter.format_transcript(transcript_data)
            
            result = {
                'text': full_text,
                'language': language_used,
                'word_count': len(full_text.split()),
                'segments': transcript_data  # ì‹œê°„ ì •ë³´ í¬í•¨
            }
            
            self._log_progress(video_id, 'transcript', 'success', 
                             {'language': language_used, 'word_count': result['word_count']})
            
            return result
            
        except Exception as e:
            self._log_progress(video_id, 'transcript', 'failed', {'error': str(e)})
            print(f"âŒ ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨ ({video_id}): {str(e)}")
            return None
    
    def transcribe_audio(self, audio_path: str, video_id: str) -> Optional[Dict]:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ STTë¡œ í…ìŠ¤íŠ¸ ë³€í™˜
        
        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            video_id: ì˜ìƒ ID
            
        Returns:
            ë³€í™˜ëœ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„°
        """
        if not self.whisper_model:
            return None
        
        try:
            self._log_progress(video_id, 'stt', 'processing', {'message': 'STT ë³€í™˜ ì‹œì‘'})
            
            # Whisperë¡œ ìŒì„± ì¸ì‹
            result = self.whisper_model.transcribe(
                audio_path,
                language='ko',  # í•œêµ­ì–´ ê°•ì œ ì§€ì •
                task='transcribe',
                word_timestamps=True  # ë‹¨ì–´ë³„ ì‹œê°„ ì •ë³´
            )
            
            transcription_data = {
                'text': result['text'],
                'language': result['language'],
                'segments': result.get('segments', []),
                'word_count': len(result['text'].split()),
                'confidence': self._calculate_confidence(result)
            }
            
            self._log_progress(video_id, 'stt', 'success', 
                             {'word_count': transcription_data['word_count'], 
                              'confidence': transcription_data['confidence']})
            
            return transcription_data
            
        except Exception as e:
            self._log_progress(video_id, 'stt', 'failed', {'error': str(e)})
            print(f"âŒ STT ë³€í™˜ ì‹¤íŒ¨ ({video_id}): {str(e)}")
            return None
    
    def _calculate_confidence(self, whisper_result: Dict) -> float:
        """Whisper ê²°ê³¼ì—ì„œ ì‹ ë¢°ë„ ê³„ì‚°"""
        if 'segments' not in whisper_result:
            return 0.5
        
        confidences = []
        for segment in whisper_result['segments']:
            if 'avg_logprob' in segment:
                # log probabilityë¥¼ 0-1 ì‹ ë¢°ë„ë¡œ ë³€í™˜
                confidence = max(0, min(1, (segment['avg_logprob'] + 1) / 1))
                confidences.append(confidence)
        
        return sum(confidences) / len(confidences) if confidences else 0.5
    
    def combine_and_analyze_content(self, video_id: str, transcript_data: Dict = None, 
                                  stt_data: Dict = None, video_info: Dict = None) -> Dict:
        """
        ìë§‰ê³¼ STT ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ì¢…í•© ë¶„ì„
        
        Args:
            video_id: ì˜ìƒ ID
            transcript_data: ìë§‰ ë°ì´í„°
            stt_data: STT ë°ì´í„°
            video_info: ì˜ìƒ ë©”íƒ€ë°ì´í„°
            
        Returns:
            í†µí•© ë¶„ì„ ê²°ê³¼
        """
        try:
            self._log_progress(video_id, 'analysis', 'processing', {'message': 'í†µí•© ë¶„ì„ ì‹œì‘'})
            
            # í…ìŠ¤íŠ¸ í†µí•©
            combined_text = ""
            text_sources = []
            
            if transcript_data:
                combined_text += transcript_data['text'] + "\n"
                text_sources.append("transcript")
            
            if stt_data:
                combined_text += stt_data['text'] + "\n"
                text_sources.append("stt")
            
            if not combined_text.strip():
                return None
            
            # ì‚¬ì£¼ ê´€ë ¨ ë‚´ìš© ë¶„ì„
            analysis = self._analyze_saju_content_advanced(combined_text)
            
            # ì§€ì‹ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
            segments = self._create_knowledge_segments(
                combined_text, transcript_data, stt_data, video_id
            )
            
            # í•™ìŠµ íŒ¨í„´ ì¶”ì¶œ
            patterns = self._extract_learning_patterns(combined_text, analysis)
            
            result = {
                'video_id': video_id,
                'combined_text': combined_text,
                'text_sources': text_sources,
                'total_word_count': len(combined_text.split()),
                'saju_analysis': analysis,
                'knowledge_segments': segments,
                'learned_patterns': patterns,
                'content_quality_score': self._calculate_content_quality(analysis, combined_text),
                'processing_summary': {
                    'has_transcript': transcript_data is not None,
                    'has_stt': stt_data is not None,
                    'transcript_quality': transcript_data.get('language') if transcript_data else None,
                    'stt_confidence': stt_data.get('confidence') if stt_data else None
                }
            }
            
            self._log_progress(video_id, 'analysis', 'success', {
                'word_count': result['total_word_count'],
                'relevance_score': analysis['relevance_score'],
                'segments_count': len(segments),
                'patterns_count': len(patterns)
            })
            
            return result
            
        except Exception as e:
            self._log_progress(video_id, 'analysis', 'failed', {'error': str(e)})
            print(f"âŒ í†µí•© ë¶„ì„ ì‹¤íŒ¨ ({video_id}): {str(e)}")
            return None
    
    def _analyze_saju_content_advanced(self, text: str) -> Dict:
        """ê³ ê¸‰ ì‚¬ì£¼ ë‚´ìš© ë¶„ì„"""
        analysis = {
            'saju_terms_found': {},
            'relevance_score': 0.0,
            'content_categories': {},
            'interpretation_patterns': [],
            'prediction_statements': [],
            'expert_indicators': []
        }
        
        # 1. ì‚¬ì£¼ ìš©ì–´ ì¶”ì¶œ ë° ë¹ˆë„ ë¶„ì„
        total_terms_found = 0
        for category, terms in self.saju_terms.items():
            found_terms = []
            for term in terms:
                count = text.count(term)
                if count > 0:
                    found_terms.append({'term': term, 'count': count, 'positions': self._find_term_positions(text, term)})
                    total_terms_found += count
            
            if found_terms:
                analysis['saju_terms_found'][category] = found_terms
        
        # 2. ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜)
        text_length = len(text.split())
        if text_length > 0:
            base_score = min(total_terms_found / text_length * 100, 10.0)
            
            # ì „ë¬¸ì„± ë³´ë„ˆìŠ¤
            expert_keywords = ['ëª…ë¦¬í•™', 'ì‚¬ì£¼í•™', 'ë™ì–‘ì² í•™', 'ì „ë¬¸ê°€', 'ì„ ìƒë‹˜', 'ë§ˆìŠ¤í„°']
            expert_bonus = sum(1 for keyword in expert_keywords if keyword in text) * 0.5
            
            # êµ¬ì²´ì„± ë³´ë„ˆìŠ¤ (êµ¬ì²´ì ì¸ í•´ì„ì´ë‚˜ ì˜ˆì¸¡ì´ ìˆëŠ”ì§€)
            specific_bonus = 0
            if any(word in text for word in ['ì˜ˆë¥¼ ë“¤ì–´', 'êµ¬ì²´ì ìœ¼ë¡œ', 'ì‹¤ì œë¡œ']):
                specific_bonus += 0.3
            
            analysis['relevance_score'] = min(base_score + expert_bonus + specific_bonus, 10.0)
        
        # 3. ë‚´ìš© ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        categories = {
            'ê¸°ì´ˆì´ë¡ ': ['ê¸°ì´ˆ', 'ì›ë¦¬', 'ê°œë…', 'ì´ë¡ '],
            'ì‹¤ì „í•´ì„': ['í•´ì„', 'ë¶„ì„', 'í’€ì´', 'ë³´ëŠ”ë²•'],
            'ìš´ì„¸ì˜ˆì¸¡': ['ìš´ì„¸', 'ì˜¬í•´', 'ë‚´ë…„', 'ì•ìœ¼ë¡œ', 'ë¯¸ë˜'],
            'ê¶í•©ë¶„ì„': ['ê¶í•©', 'ì¸ì—°', 'ê²°í˜¼', 'ì—°ì• '],
            'ì§ì—…ì§„ë¡œ': ['ì§ì—…', 'ì§„ë¡œ', 'ì‚¬ì—…', 'ì ì„±'],
            'ê±´ê°•': ['ê±´ê°•', 'ëª¸', 'ì§ˆë³‘', 'ì£¼ì˜'],
            'ì¬ë¬¼': ['ëˆ', 'ì¬ë¬¼', 'ì¬ìš´', 'íˆ¬ì', 'ì‚¬ì—…']
        }
        
        for category, keywords in categories.items():
            score = sum(text.lower().count(keyword) for keyword in keywords)
            if score > 0:
                analysis['content_categories'][category] = score
        
        # 4. í•´ì„ íŒ¨í„´ ì¶”ì¶œ
        interpretation_patterns = re.findall(
            r'([ê°€-í£\s]+ì´ë©´|[ê°€-í£\s]+ì¼ë•Œ|[ê°€-í£\s]+ê²½ìš°)\s*([ê°€-í£\s,\.]+[ë‹¤ë‹ˆ])', 
            text
        )
        analysis['interpretation_patterns'] = [
            {'condition': cond.strip(), 'result': res.strip()} 
            for cond, res in interpretation_patterns 
            if self._contains_saju_terms(cond) or self._contains_saju_terms(res)
        ]
        
        # 5. ì˜ˆì¸¡ ì§„ìˆ  ì¶”ì¶œ
        prediction_patterns = re.findall(
            r'(ì˜¬í•´|ë‚´ë…„|ì•ìœ¼ë¡œ|ë¯¸ë˜ì—)\s*([ê°€-í£\s,\.]+[ë‹¤ë‹ˆ])', 
            text
        )
        analysis['prediction_statements'] = [
            {'timeframe': time.strip(), 'prediction': pred.strip()}
            for time, pred in prediction_patterns
        ]
        
        return analysis
    
    def _find_term_positions(self, text: str, term: str) -> List[int]:
        """í…ìŠ¤íŠ¸ì—ì„œ ìš©ì–´ì˜ ìœ„ì¹˜ë“¤ì„ ì°¾ê¸°"""
        positions = []
        start = 0
        while True:
            pos = text.find(term, start)
            if pos == -1:
                break
            positions.append(pos)
            start = pos + len(term)
        return positions
    
    def _create_knowledge_segments(self, combined_text: str, transcript_data: Dict, 
                                 stt_data: Dict, video_id: str) -> List[Dict]:
        """ì§€ì‹ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±"""
        segments = []
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = self._split_into_sentences(combined_text)
        
        for i, sentence in enumerate(sentences):
            if len(sentence.strip()) < 10:
                continue
                
            if self._is_saju_knowledge_sentence(sentence):
                segment = {
                    'text': sentence.strip(),
                    'sequence_number': i,
                    'saju_terms': self._extract_terms_from_sentence(sentence),
                    'category': self._classify_knowledge_type(sentence),
                    'confidence': self._calculate_segment_confidence(sentence),
                    'embedding': self._get_sentence_embedding(sentence)
                }
                
                segments.append(segment)
        
        return segments
    
    def _extract_learning_patterns(self, text: str, analysis: Dict) -> List[Dict]:
        """í•™ìŠµ íŒ¨í„´ ì¶”ì¶œ"""
        patterns = []
        
        # í•´ì„ íŒ¨í„´ë“¤ì„ í•™ìŠµìš©ìœ¼ë¡œ ë³€í™˜
        for pattern in analysis['interpretation_patterns']:
            learning_pattern = {
                'type': 'í•´ì„íŒ¨í„´',
                'condition': pattern['condition'],
                'result': pattern['result'],
                'confidence': 0.7,
                'source_context': self._extract_context(text, pattern['condition'])
            }
            patterns.append(learning_pattern)
        
        # ì˜ˆì¸¡ íŒ¨í„´ë“¤ë„ ì¶”ê°€
        for prediction in analysis['prediction_statements']:
            learning_pattern = {
                'type': 'ì˜ˆì¸¡íŒ¨í„´',
                'timeframe': prediction['timeframe'],
                'prediction': prediction['prediction'],
                'confidence': 0.6,
                'source_context': self._extract_context(text, prediction['prediction'])
            }
            patterns.append(learning_pattern)
        
        return patterns
    
    def _extract_context(self, text: str, target_phrase: str, context_words: int = 20) -> str:
        """ëŒ€ìƒ êµ¬ë¬¸ ì£¼ë³€ì˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        words = text.split()
        target_words = target_phrase.split()
        
        for i in range(len(words) - len(target_words) + 1):
            if ' '.join(words[i:i+len(target_words)]) == target_phrase:
                start = max(0, i - context_words)
                end = min(len(words), i + len(target_words) + context_words)
                return ' '.join(words[start:end])
        
        return target_phrase
    
    def _calculate_content_quality(self, analysis: Dict, text: str) -> float:
        """ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        quality_score = 0.0
        
        # 1. ì‚¬ì£¼ ê´€ë ¨ì„± (40%)
        quality_score += analysis['relevance_score'] * 0.4
        
        # 2. ë‚´ìš© ë‹¤ì–‘ì„± (30%)
        category_diversity = len(analysis['content_categories']) / 7.0  # ìµœëŒ€ 7ê°œ ì¹´í…Œê³ ë¦¬
        quality_score += category_diversity * 3.0
        
        # 3. êµ¬ì²´ì„± (20%)
        specific_indicators = ['ì˜ˆë¥¼ ë“¤ì–´', 'êµ¬ì²´ì ìœ¼ë¡œ', 'ì‹¤ì œë¡œ', 'ì˜ˆì‹œ', 'ì‚¬ë¡€']
        specificity = sum(1 for indicator in specific_indicators if indicator in text) / len(specific_indicators)
        quality_score += specificity * 2.0
        
        # 4. êµ¬ì¡°í™” ì •ë„ (10%)
        structure_indicators = len(analysis['interpretation_patterns']) + len(analysis['prediction_statements'])
        structure_score = min(structure_indicators / 5.0, 1.0)  # ìµœëŒ€ 5ê°œ íŒ¨í„´
        quality_score += structure_score * 1.0
        
        return min(quality_score, 10.0)
    
    def _get_sentence_embedding(self, sentence: str) -> Optional[bytes]:
        """ë¬¸ì¥ ì„ë² ë”© ìƒì„±"""
        if not self.embedding_model:
            return None
        
        try:
            embedding = self.embedding_model.encode(sentence)
            return embedding.tobytes()
        except:
            return None
    
    def _calculate_segment_confidence(self, sentence: str) -> float:
        """ì„¸ê·¸ë¨¼íŠ¸ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.5
        
        # ì‚¬ì£¼ ìš©ì–´ ë°€ë„
        saju_term_count = sum(1 for terms in self.saju_terms.values() 
                             for term in terms if term in sentence)
        term_density = saju_term_count / len(sentence.split())
        confidence += min(term_density * 2, 0.3)
        
        # êµ¬ì²´ì„±
        if any(word in sentence for word in ['êµ¬ì²´ì ìœ¼ë¡œ', 'ì˜ˆë¥¼ ë“¤ì–´', 'ì‹¤ì œë¡œ']):
            confidence += 0.2
        
        return min(confidence, 1.0)
    
    def save_complete_analysis(self, analysis_result: Dict):
        """ì™„ì „í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(self.knowledge_db_path)
        cursor = conn.cursor()
        
        try:
            video_id = analysis_result['video_id']
            
            # 1. ì˜ìƒ ì½˜í…ì¸  ë©”ì¸ ì •ë³´ ì €ì¥
            cursor.execute('''
                INSERT OR REPLACE INTO video_content 
                (video_id, combined_text, total_word_count, saju_relevance_score, 
                 content_quality_score, has_transcript, has_audio_extracted)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                video_id,
                analysis_result['combined_text'],
                analysis_result['total_word_count'],
                analysis_result['saju_analysis']['relevance_score'],
                analysis_result['content_quality_score'],
                analysis_result['processing_summary']['has_transcript'],
                analysis_result['processing_summary']['has_stt']
            ))
            
            # 2. ì§€ì‹ ì„¸ê·¸ë¨¼íŠ¸ë“¤ ì €ì¥
            for segment in analysis_result['knowledge_segments']:
                cursor.execute('''
                    INSERT INTO knowledge_segments 
                    (video_id, segment_text, saju_terms, knowledge_category, confidence_score, embedding)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    video_id,
                    segment['text'],
                    json.dumps(segment['saju_terms'], ensure_ascii=False),
                    segment['category'],
                    segment['confidence'],
                    segment['embedding']
                ))
            
            # 3. í•™ìŠµëœ íŒ¨í„´ë“¤ ì €ì¥
            for pattern in analysis_result['learned_patterns']:
                cursor.execute('''
                    INSERT INTO saju_knowledge_patterns 
                    (pattern_type, condition_elements, result_interpretation, confidence_level, source_videos)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    pattern['type'],
                    pattern.get('condition', ''),
                    pattern.get('result', pattern.get('prediction', '')),
                    pattern['confidence'],
                    json.dumps([video_id])
                ))
            
            # 4. ì‚¬ì£¼ ìš©ì–´ ì‚¬ì „ ì—…ë°ì´íŠ¸
            for category, terms in analysis_result['saju_analysis']['saju_terms_found'].items():
                for term_data in terms:
                    cursor.execute('''
                        INSERT OR IGNORE INTO saju_dictionary (term, category, frequency)
                        VALUES (?, ?, 0)
                    ''', (term_data['term'], category))
                    
                    cursor.execute('''
                        UPDATE saju_dictionary 
                        SET frequency = frequency + ?, updated_at = CURRENT_TIMESTAMP
                        WHERE term = ?
                    ''', (term_data['count'], term_data['term']))
            
            conn.commit()
            self._log_progress(video_id, 'learning', 'success', {
                'segments_saved': len(analysis_result['knowledge_segments']),
                'patterns_saved': len(analysis_result['learned_patterns']),
                'relevance_score': analysis_result['saju_analysis']['relevance_score']
            })
            
            print(f"âœ… í•™ìŠµ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {video_id}")
            
        except Exception as e:
            conn.rollback()
            self._log_progress(video_id, 'learning', 'failed', {'error': str(e)})
            print(f"âŒ í•™ìŠµ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ ({video_id}): {str(e)}")
        finally:
            conn.close()
    
    def process_complete_video(self, video_id: str, video_info: Dict = None) -> bool:
        """
        ì˜ìƒ ì „ì²´ ì²˜ë¦¬: ë‹¤ìš´ë¡œë“œ â†’ ìë§‰ì¶”ì¶œ â†’ STT â†’ í†µí•©ë¶„ì„ â†’ í•™ìŠµ
        
        Args:
            video_id: YouTube ì˜ìƒ ID  
            video_info: ì˜ìƒ ë©”íƒ€ë°ì´í„°
            
        Returns:
            ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€
        """
        print(f"\nğŸ¬ ì˜ìƒ ì „ì²´ ì²˜ë¦¬ ì‹œì‘: {video_id}")
        
        try:
            # 1. ìë§‰ ì¶”ì¶œ
            transcript_data = self.extract_transcript(video_id)
            if transcript_data:
                print(f"âœ… ìë§‰ ì¶”ì¶œ: {transcript_data['word_count']}ë‹¨ì–´ ({transcript_data['language']})")
            
            # 2. ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
            audio_path = self.download_audio(video_id)
            stt_data = None
            
            if audio_path:
                print(f"âœ… ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ: {os.path.getsize(audio_path)/(1024*1024):.1f}MB")
                
                # 3. STT ë³€í™˜
                stt_data = self.transcribe_audio(audio_path, video_id)
                if stt_data:
                    print(f"âœ… STT ë³€í™˜: {stt_data['word_count']}ë‹¨ì–´ (ì‹ ë¢°ë„: {stt_data['confidence']:.2f})")
                
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                try:
                    os.remove(audio_path)
                except:
                    pass
            
            # ìë§‰ë„ STTë„ ì—†ìœ¼ë©´ ì‹¤íŒ¨
            if not transcript_data and not stt_data:
                print(f"âŒ ìë§‰ê³¼ STT ëª¨ë‘ ì‹¤íŒ¨: {video_id}")
                return False
            
            # 4. í†µí•© ë¶„ì„
            analysis_result = self.combine_and_analyze_content(
                video_id, transcript_data, stt_data, video_info
            )
            
            if not analysis_result:
                print(f"âŒ í†µí•© ë¶„ì„ ì‹¤íŒ¨: {video_id}")
                return False
            
            print(f"ğŸ“Š ë¶„ì„ ì™„ë£Œ:")
            print(f"  - ì´ ë‹¨ì–´ ìˆ˜: {analysis_result['total_word_count']}")
            print(f"  - ì‚¬ì£¼ ê´€ë ¨ì„±: {analysis_result['saju_analysis']['relevance_score']:.2f}/10")
            print(f"  - ì½˜í…ì¸  í’ˆì§ˆ: {analysis_result['content_quality_score']:.2f}/10")
            print(f"  - ì§€ì‹ ì„¸ê·¸ë¨¼íŠ¸: {len(analysis_result['knowledge_segments'])}ê°œ")
            print(f"  - í•™ìŠµëœ íŒ¨í„´: {len(analysis_result['learned_patterns'])}ê°œ")
            
            # ê´€ë ¨ì„±ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if analysis_result['saju_analysis']['relevance_score'] < 0.5:
                print(f"âš ï¸ ê´€ë ¨ì„±ì´ ë„ˆë¬´ ë‚®ì•„ í•™ìŠµí•˜ì§€ ì•ŠìŒ")
                return False
            
            # 5. ë°ì´í„°ë² ì´ìŠ¤ì— í•™ìŠµ ê²°ê³¼ ì €ì¥
            self.save_complete_analysis(analysis_result)
            
            return True
            
        except Exception as e:
            print(f"âŒ ì˜ìƒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({video_id}): {str(e)}")
            return False
    
    def batch_learn_from_videos(self, video_list: List[Dict], max_videos: int = 10) -> Dict:
        """
        ì—¬ëŸ¬ ì˜ìƒì—ì„œ ì¼ê´„ í•™ìŠµ
        
        Args:
            video_list: ì˜ìƒ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            max_videos: ìµœëŒ€ ì²˜ë¦¬ ì˜ìƒ ìˆ˜
            
        Returns:
            í•™ìŠµ ê²°ê³¼ í†µê³„
        """
        results = {
            'total_videos': min(len(video_list), max_videos),
            'success_count': 0,
            'failed_count': 0,
            'total_knowledge_segments': 0,
            'total_patterns_learned': 0,
            'avg_relevance_score': 0.0,
            'avg_quality_score': 0.0,
            'processing_time': 0
        }
        
        start_time = datetime.now()
        processed_videos = video_list[:max_videos]
        
        print(f"ğŸš€ ì¼ê´„ í•™ìŠµ ì‹œì‘: {len(processed_videos)}ê°œ ì˜ìƒ")
        print("=" * 60)
        
        relevance_scores = []
        quality_scores = []
        
        for i, video_info in enumerate(processed_videos, 1):
            print(f"\n[{i}/{len(processed_videos)}] ì²˜ë¦¬ ì¤‘...")
            
            video_id = video_info.get('video_id')
            if not video_id:
                results['failed_count'] += 1
                continue
            
            success = self.process_complete_video(video_id, video_info)
            
            if success:
                results['success_count'] += 1
                
                # í†µê³„ ìˆ˜ì§‘
                conn = sqlite3.connect(self.knowledge_db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT saju_relevance_score, content_quality_score 
                    FROM video_content WHERE video_id = ?
                ''', (video_id,))
                row = cursor.fetchone()
                if row:
                    relevance_scores.append(row[0])
                    quality_scores.append(row[1])
                
                conn.close()
            else:
                results['failed_count'] += 1
        
        # ìµœì¢… í†µê³„ ê³„ì‚°
        end_time = datetime.now()
        results['processing_time'] = (end_time - start_time).total_seconds()
        
        if relevance_scores:
            results['avg_relevance_score'] = sum(relevance_scores) / len(relevance_scores)
        if quality_scores:
            results['avg_quality_score'] = sum(quality_scores) / len(quality_scores)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ í†µê³„
        conn = sqlite3.connect(self.knowledge_db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM knowledge_segments')
        results['total_knowledge_segments'] = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM saju_knowledge_patterns')
        results['total_patterns_learned'] = cursor.fetchone()[0]
        
        conn.close()
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n" + "=" * 60)
        print(f"ğŸ‰ ì¼ê´„ í•™ìŠµ ì™„ë£Œ!")
        print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {results['processing_time']:.1f}ì´ˆ")
        print(f"âœ… ì„±ê³µ: {results['success_count']}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {results['failed_count']}ê°œ")
        print(f"ğŸ“Š í‰ê·  ì‚¬ì£¼ ê´€ë ¨ì„±: {results['avg_relevance_score']:.2f}/10")
        print(f"ğŸ¯ í‰ê·  ì½˜í…ì¸  í’ˆì§ˆ: {results['avg_quality_score']:.2f}/10")
        print(f"ğŸ§  ì´ ì§€ì‹ ì„¸ê·¸ë¨¼íŠ¸: {results['total_knowledge_segments']}ê°œ")
        print(f"ğŸ”„ ì´ í•™ìŠµ íŒ¨í„´: {results['total_patterns_learned']}ê°œ")
        
        return results
    
    def get_learned_knowledge_summary(self) -> Dict:
        """í•™ìŠµëœ ì§€ì‹ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        conn = sqlite3.connect(self.knowledge_db_path)
        cursor = conn.cursor()
        
        summary = {}
        
        # ê¸°ë³¸ í†µê³„
        cursor.execute('SELECT COUNT(*) FROM video_content WHERE saju_relevance_score > 0')
        summary['total_processed_videos'] = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM knowledge_segments')
        summary['total_knowledge_segments'] = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM saju_knowledge_patterns')
        summary['total_patterns'] = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM saju_dictionary WHERE frequency > 0')
        summary['learned_terms_count'] = cursor.fetchone()[0]
        
        # í’ˆì§ˆ í†µê³„
        cursor.execute('SELECT AVG(saju_relevance_score), AVG(content_quality_score) FROM video_content')
        row = cursor.fetchone()
        summary['avg_relevance_score'] = row[0] or 0
        summary['avg_quality_score'] = row[1] or 0
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì§€ì‹ ë¶„í¬
        cursor.execute('''
            SELECT knowledge_category, COUNT(*) 
            FROM knowledge_segments 
            GROUP BY knowledge_category 
            ORDER BY COUNT(*) DESC
        ''')
        summary['knowledge_by_category'] = dict(cursor.fetchall())
        
        # ìì£¼ ë‚˜ì˜¤ëŠ” ì‚¬ì£¼ ìš©ì–´ TOP 10
        cursor.execute('''
            SELECT term, frequency 
            FROM saju_dictionary 
            WHERE frequency > 0 
            ORDER BY frequency DESC 
            LIMIT 10
        ''')
        summary['top_saju_terms'] = dict(cursor.fetchall())
        
        conn.close()
        return summary
    
    def _log_progress(self, video_id: str, stage: str, status: str, details: Dict):
        """ì²˜ë¦¬ ê³¼ì • ë¡œê·¸ ì €ì¥"""
        conn = sqlite3.connect(self.knowledge_db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO learning_progress 
            (video_id, processing_stage, status, details)
            VALUES (?, ?, ?, ?)
        ''', (video_id, stage, status, json.dumps(details, ensure_ascii=False)))
        
        conn.commit()
        conn.close()
    
    def cleanup_temp_files(self):
        """ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬"""
        try:
            shutil.rmtree(self.temp_dir)
            print("ğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
        except:
            pass
    
    def __del__(self):
        """ì†Œë©¸ìì—ì„œ ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        self.cleanup_temp_files()

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ (ìœ ì§€)
    def _split_into_sentences(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„í• """
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', text)
        return [s.strip() for s in sentences if len(s.strip()) > 10]
    
    def _is_saju_knowledge_sentence(self, sentence: str) -> bool:
        """ë¬¸ì¥ì´ ì‚¬ì£¼ ì§€ì‹ì„ í¬í•¨í•˜ëŠ”ì§€ íŒë‹¨"""
        saju_indicator_words = [
            'ì‚¬ì£¼', 'íŒ”ì', 'ìš´ì„¸', 'ëª…ë¦¬', 'ì˜¤í–‰', 'ì²œê°„', 'ì§€ì§€',
            'ê°‘ì„ë³‘ì •', 'ìì¶•ì¸ë¬˜', 'ëŒ€ìš´', 'ì„¸ìš´', 'ì¼ê°„', 'ì›”ë ¹',
            'í•´ì„', 'ë¶„ì„', 'ì˜ë¯¸', 'íŠ¹ì§•', 'ì„±ê²©', 'ìš´ëª…'
        ]
        
        sentence_lower = sentence.lower()
        return any(word in sentence_lower for word in saju_indicator_words)
    
    def _extract_terms_from_sentence(self, sentence: str) -> List[str]:
        """ë¬¸ì¥ì—ì„œ ì‚¬ì£¼ ìš©ì–´ ì¶”ì¶œ"""
        found_terms = []
        for category, terms in self.saju_terms.items():
            for term in terms:
                if term in sentence:
                    found_terms.append(term)
        return found_terms
    
    def _classify_knowledge_type(self, sentence: str) -> str:
        """ì§€ì‹ ìœ í˜• ë¶„ë¥˜"""
        sentence_lower = sentence.lower()
        
        if any(word in sentence_lower for word in ['ì˜ˆì¸¡', 'ì•ìœ¼ë¡œ', 'ë¯¸ë˜', 'ì˜¬í•´', 'ë‚´ë…„']):
            return 'ì˜ˆì¸¡'
        elif any(word in sentence_lower for word in ['í•´ì„', 'ì˜ë¯¸', 'ëœ»', 'ë‚˜íƒ€ë‚´']):
            return 'í•´ì„'
        elif any(word in sentence_lower for word in ['ì„±ê²©', 'ì„±í–¥', 'íŠ¹ì§•', 'ê¸°ì§ˆ']):
            return 'ì„±ê²©ë¶„ì„'
        elif any(word in sentence_lower for word in ['ê¶í•©', 'ì¸ì—°', 'ê²°í˜¼', 'ì—°ì• ']):
            return 'ê´€ê³„'
        elif any(word in sentence_lower for word in ['ì§ì—…', 'ì§„ë¡œ', 'ì‚¬ì—…', 'ëˆ', 'ì¬ë¬¼']):
            return 'ì§„ë¡œì¬ë¬¼'
        else:
            return 'ì¼ë°˜'
    
    def _contains_saju_terms(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ì— ì‚¬ì£¼ ìš©ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        for terms in self.saju_terms.values():
            if any(term in text for term in terms):
                return True
        return False

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ¬ YouTube ì˜ìƒ ì™„ì „ ë¶„ì„ ë° í•™ìŠµ ì‹œìŠ¤í…œ ì‹œì‘")
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = YouTubeContentAnalyzer()
    
    # ì‚¬ì£¼ ì˜ìƒ ê²€ìƒ‰
    crawler = YouTubeSajuCrawler()
    print("\nğŸ” ì‚¬ì£¼ ê´€ë ¨ ì˜ìƒ ê²€ìƒ‰ ì¤‘...")
    videos = crawler.crawl_saju_videos(max_per_keyword=2)  # í…ŒìŠ¤íŠ¸ìš©
    
    if not videos:
        print("âŒ ê²€ìƒ‰ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… {len(videos)}ê°œ ì˜ìƒ ë°œê²¬")
    
    # ì˜ìƒë“¤ ì™„ì „ ë¶„ì„ ë° í•™ìŠµ
    results = analyzer.batch_learn_from_videos(videos, max_videos=3)
    
    # í•™ìŠµëœ ì§€ì‹ ìš”ì•½
    summary = analyzer.get_learned_knowledge_summary()
    
    print(f"\nğŸ“š í•™ìŠµëœ ì§€ì‹ ìš”ì•½:")
    print(f"  - ì²˜ë¦¬ëœ ì˜ìƒ: {summary['total_processed_videos']}ê°œ")
    print(f"  - ì§€ì‹ ì„¸ê·¸ë¨¼íŠ¸: {summary['total_knowledge_segments']}ê°œ")
    print(f"  - í•™ìŠµëœ íŒ¨í„´: {summary['total_patterns']}ê°œ")
    print(f"  - ì‚¬ì£¼ ìš©ì–´: {summary['learned_terms_count']}ê°œ")
    print(f"  - í‰ê·  í’ˆì§ˆ: {summary['avg_quality_score']:.2f}/10")
    
    # ìƒìœ„ ì‚¬ì£¼ ìš©ì–´ë“¤ ì¶œë ¥
    if summary['top_saju_terms']:
        print(f"\nğŸ”¥ ìì£¼ ë‚˜ì˜¤ëŠ” ì‚¬ì£¼ ìš©ì–´:")
        for term, freq in list(summary['top_saju_terms'].items())[:5]:
            print(f"  - {term}: {freq}íšŒ")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì§€ì‹ ë¶„í¬
    if summary['knowledge_by_category']:
        print(f"\nğŸ“‚ ì§€ì‹ ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
        for category, count in list(summary['knowledge_by_category'].items())[:5]:
            print(f"  - {category}: {count}ê°œ")
    
    # ì •ë¦¬
    analyzer.cleanup_temp_files()
    print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")

if __name__ == "__main__":
    main()