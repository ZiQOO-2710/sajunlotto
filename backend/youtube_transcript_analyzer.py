#!/usr/bin/env python3
"""
ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ ë° ì‚¬ì£¼ ì§€ì‹ í•™ìŠµ ì‹œìŠ¤í…œ
YouTube ì˜ìƒì˜ ìë§‰ì„ ë¶„ì„í•˜ì—¬ ì‚¬ì£¼ ê´€ë ¨ ì§€ì‹ì„ ì¶”ì¶œí•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤.
"""

import os
import re
import json
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import sqlite3
from pathlib import Path

try:
    from youtube_transcript_api import YouTubeTranscriptApi
    from youtube_transcript_api.formatters import TextFormatter
except ImportError:
    print("youtube-transcript-api not installed. Install with: pip install youtube-transcript-api")

try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
except ImportError:
    print("sentence-transformers not installed. Install with: pip install sentence-transformers")

from youtube_crawler import YouTubeSajuCrawler

class SajuTranscriptAnalyzer:
    """ì‚¬ì£¼ ê´€ë ¨ YouTube ìë§‰ ë¶„ì„ ë° í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self, knowledge_db_path: str = "saju_knowledge.db"):
        """
        ì´ˆê¸°í™”
        Args:
            knowledge_db_path: ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
        """
        self.knowledge_db_path = knowledge_db_path
        self.setup_knowledge_database()
        
        # ì‚¬ì£¼ ê´€ë ¨ í•µì‹¬ ìš©ì–´ë“¤
        self.saju_terms = {
            'ì²œê°„': ['ê°‘', 'ì„', 'ë³‘', 'ì •', 'ë¬´', 'ê¸°', 'ê²½', 'ì‹ ', 'ì„', 'ê³„'],
            'ì§€ì§€': ['ì', 'ì¶•', 'ì¸', 'ë¬˜', 'ì§„', 'ì‚¬', 'ì˜¤', 'ë¯¸', 'ì‹ ', 'ìœ ', 'ìˆ ', 'í•´'],
            'ì˜¤í–‰': ['ëª©', 'í™”', 'í† ', 'ê¸ˆ', 'ìˆ˜'],
            'ìŒì–‘': ['ì–‘', 'ìŒ', 'ì–‘ê°„', 'ìŒê°„'],
            'ì‹­ì‹ ': ['ë¹„ê²¬', 'ê²ì¬', 'ì‹ì‹ ', 'ìƒê´€', 'í¸ì¬', 'ì •ì¬', 'í¸ê´€', 'ì •ê´€', 'í¸ì¸', 'ì •ì¸'],
            'ìš´ì„¸_ê°œë…': ['ëŒ€ìš´', 'ì„¸ìš´', 'ì›”ìš´', 'ì¼ìš´', 'ì‹ ì‚´', 'ê³µë§', 'í˜•ì¶©íŒŒí•´'],
            'ì‚¬ì£¼_êµ¬ì„±': ['ë…„ì£¼', 'ì›”ì£¼', 'ì¼ì£¼', 'ì‹œì£¼', 'ì¼ê°„', 'ì›”ë ¹']
        }
        
        # ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ (í•œêµ­ì–´ ì§€ì›)
        try:
            self.embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        except:
            print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ê¸°ë³¸ ë¶„ì„ ê¸°ëŠ¥ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
            self.embedding_model = None
    
    def setup_knowledge_database(self):
        """ì§€ì‹ ë² ì´ìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.knowledge_db_path)
        cursor = conn.cursor()
        
        # ìë§‰ ì›ë³¸ ë°ì´í„° í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS video_transcripts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                video_id TEXT UNIQUE NOT NULL,
                title TEXT,
                channel_title TEXT,
                transcript_text TEXT,
                language TEXT DEFAULT 'ko',
                extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                word_count INTEGER,
                saju_relevance_score REAL
            )
        ''')
        
        # ì¶”ì¶œëœ ì‚¬ì£¼ ì§€ì‹ ì¡°ê°ë“¤
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS saju_knowledge_chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                video_id TEXT,
                chunk_text TEXT,
                saju_terms TEXT, -- JSON array of extracted terms
                knowledge_category TEXT, -- ì˜ˆì¸¡, í•´ì„, ì´ë¡ , ì‹¤ì „ ë“±
                confidence_score REAL,
                embedding BLOB, -- sentence embedding
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (video_id) REFERENCES video_transcripts (video_id)
            )
        ''')
        
        # ì‚¬ì£¼ ìš©ì–´ ì‚¬ì „
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS saju_terminology (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                term TEXT UNIQUE NOT NULL,
                category TEXT, -- ì²œê°„, ì§€ì§€, ì˜¤í–‰ ë“±
                definition TEXT,
                examples TEXT, -- JSON array
                frequency INTEGER DEFAULT 0,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # í•™ìŠµëœ íŒ¨í„´ë“¤
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learned_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern_type TEXT, -- í•´ì„_íŒ¨í„´, ì˜ˆì¸¡_íŒ¨í„´ ë“±
                condition_terms TEXT, -- JSON: ì¡°ê±´ì´ ë˜ëŠ” ì‚¬ì£¼ ìš”ì†Œë“¤
                result_description TEXT, -- ê²°ê³¼ ì„¤ëª…
                confidence REAL,
                source_count INTEGER DEFAULT 1, -- ì´ íŒ¨í„´ì„ ë’·ë°›ì¹¨í•˜ëŠ” ì†ŒìŠ¤ ê°œìˆ˜
                examples TEXT, -- JSON: ì‹¤ì œ ì˜ˆì‹œë“¤
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def extract_transcript(self, video_id: str) -> Optional[str]:
        """
        YouTube ì˜ìƒì˜ ìë§‰ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            video_id: YouTube ì˜ìƒ ID
            
        Returns:
            ì¶”ì¶œëœ ìë§‰ í…ìŠ¤íŠ¸ ë˜ëŠ” None
        """
        try:
            # í•œêµ­ì–´ ìë§‰ ìš°ì„ , ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì–¸ì–´
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            
            transcript = None
            language_used = None
            
            # í•œêµ­ì–´ ìë§‰ ì°¾ê¸°
            try:
                transcript = transcript_list.find_transcript(['ko'])
                language_used = 'ko'
            except:
                # í•œêµ­ì–´ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„± ìë§‰ì´ë‚˜ ë‹¤ë¥¸ ì–¸ì–´
                try:
                    transcript = transcript_list.find_generated_transcript(['ko'])
                    language_used = 'ko-generated'
                except:
                    # ì˜ì–´ë¼ë„ ìˆìœ¼ë©´ ê°€ì ¸ì˜¤ê¸°
                    try:
                        transcript = transcript_list.find_transcript(['en'])
                        language_used = 'en'
                    except:
                        return None
            
            # ìë§‰ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            transcript_data = transcript.fetch()
            formatter = TextFormatter()
            transcript_text = formatter.format_transcript(transcript_data)
            
            return {
                'text': transcript_text,
                'language': language_used,
                'word_count': len(transcript_text.split())
            }
            
        except Exception as e:
            print(f"ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨ ({video_id}): {str(e)}")
            return None
    
    def analyze_saju_content(self, text: str) -> Dict:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ì£¼ ê´€ë ¨ ë‚´ìš©ì„ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        analysis = {
            'saju_terms_found': {},
            'relevance_score': 0.0,
            'knowledge_chunks': [],
            'patterns': []
        }
        
        text_lower = text.lower()
        
        # 1. ì‚¬ì£¼ ìš©ì–´ ì¶”ì¶œ
        total_terms_found = 0
        for category, terms in self.saju_terms.items():
            found_terms = []
            for term in terms:
                if term in text:
                    count = text.count(term)
                    found_terms.append({'term': term, 'count': count})
                    total_terms_found += count
            
            if found_terms:
                analysis['saju_terms_found'][category] = found_terms
        
        # 2. ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        text_length = len(text.split())
        if text_length > 0:
            analysis['relevance_score'] = min(total_terms_found / text_length * 100, 10.0)
        
        # 3. ì§€ì‹ ë©ì–´ë¦¬ ì¶”ì¶œ (ë¬¸ì¥ ë‹¨ìœ„)
        sentences = self._split_into_sentences(text)
        for sentence in sentences:
            if self._is_saju_knowledge_sentence(sentence):
                chunk = {
                    'text': sentence.strip(),
                    'terms': self._extract_terms_from_sentence(sentence),
                    'category': self._classify_knowledge_type(sentence)
                }
                analysis['knowledge_chunks'].append(chunk)
        
        # 4. íŒ¨í„´ ë¶„ì„ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
        patterns = self._extract_patterns(text)
        analysis['patterns'] = patterns
        
        return analysis
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„í• """
        # í•œêµ­ì–´ ë¬¸ì¥ êµ¬ë¶„ì
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', text)
        return [s.strip() for s in sentences if len(s.strip()) > 10]
    
    def _is_saju_knowledge_sentence(self, sentence: str) -> bool:
        """ë¬¸ì¥ì´ ì‚¬ì£¼ ì§€ì‹ì„ í¬í•¨í•˜ëŠ”ì§€ íŒë‹¨"""
        saju_indicator_words = [
            'ì‚¬ì£¼', 'íŒ”ì', 'ìš´ì„¸', 'ëª…ë¦¬', 'ì˜¤í–‰', 'ì²œê°„', 'ì§€ì§€',
            'ê°‘ì„ë³‘ì •', 'ìì¶•ì¸ë¬˜', 'ëŒ€ìš´', 'ì„¸ìš´', 'ì¼ê°„', 'ì›”ë ¹',
            'í•´ì„', 'ë¶„ì„', 'ì˜ë¯¸', 'íŠ¹ì§•', 'ì„±ê²©', 'ìš´ëª…'
        ]
        
        sentence_lower = sentence.lower()
        return any(word in sentence_lower for word in saju_indicator_words)
    
    def _extract_terms_from_sentence(self, sentence: str) -> List[str]:
        """ë¬¸ì¥ì—ì„œ ì‚¬ì£¼ ìš©ì–´ ì¶”ì¶œ"""
        found_terms = []
        for category, terms in self.saju_terms.items():
            for term in terms:
                if term in sentence:
                    found_terms.append(term)
        return found_terms
    
    def _classify_knowledge_type(self, sentence: str) -> str:
        """ì§€ì‹ ìœ í˜• ë¶„ë¥˜"""
        sentence_lower = sentence.lower()
        
        if any(word in sentence_lower for word in ['ì˜ˆì¸¡', 'ì•ìœ¼ë¡œ', 'ë¯¸ë˜', 'ì˜¬í•´', 'ë‚´ë…„']):
            return 'ì˜ˆì¸¡'
        elif any(word in sentence_lower for word in ['í•´ì„', 'ì˜ë¯¸', 'ëœ»', 'ë‚˜íƒ€ë‚´']):
            return 'í•´ì„'
        elif any(word in sentence_lower for word in ['ì„±ê²©', 'ì„±í–¥', 'íŠ¹ì§•', 'ê¸°ì§ˆ']):
            return 'ì„±ê²©ë¶„ì„'
        elif any(word in sentence_lower for word in ['ê¶í•©', 'ì¸ì—°', 'ê²°í˜¼', 'ì—°ì• ']):
            return 'ê´€ê³„'
        elif any(word in sentence_lower for word in ['ì§ì—…', 'ì§„ë¡œ', 'ì‚¬ì—…', 'ëˆ', 'ì¬ë¬¼']):
            return 'ì§„ë¡œì¬ë¬¼'
        else:
            return 'ì¼ë°˜'
    
    def _extract_patterns(self, text: str) -> List[Dict]:
        """ê°„ë‹¨í•œ íŒ¨í„´ ì¶”ì¶œ"""
        patterns = []
        
        # "Aì´ë©´ Bë‹¤" í˜•íƒœì˜ íŒ¨í„´ ì°¾ê¸°
        condition_patterns = re.findall(r'([ê°€-í£\s]+ì´ë©´|[ê°€-í£\s]+ì¼ë•Œ|[ê°€-í£\s]+ê²½ìš°)\s*([ê°€-í£\s,]+[ë‹¤ë‹ˆ])', text)
        
        for condition, result in condition_patterns:
            if self._contains_saju_terms(condition) or self._contains_saju_terms(result):
                patterns.append({
                    'type': 'ì¡°ê±´ë¶€_í•´ì„',
                    'condition': condition.strip(),
                    'result': result.strip(),
                    'confidence': 0.7
                })
        
        return patterns
    
    def _contains_saju_terms(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ì— ì‚¬ì£¼ ìš©ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        for terms in self.saju_terms.values():
            if any(term in text for term in terms):
                return True
        return False
    
    def save_transcript_analysis(self, video_id: str, video_info: Dict, transcript_data: Dict, analysis: Dict):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(self.knowledge_db_path)
        cursor = conn.cursor()
        
        try:
            # 1. ìë§‰ ì›ë³¸ ì €ì¥
            cursor.execute('''
                INSERT OR REPLACE INTO video_transcripts 
                (video_id, title, channel_title, transcript_text, language, word_count, saju_relevance_score)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                video_id,
                video_info.get('title', ''),
                video_info.get('channel_title', ''),
                transcript_data['text'],
                transcript_data['language'],
                transcript_data['word_count'],
                analysis['relevance_score']
            ))
            
            # 2. ì§€ì‹ ë©ì–´ë¦¬ë“¤ ì €ì¥
            for chunk in analysis['knowledge_chunks']:
                embedding = None
                if self.embedding_model:
                    try:
                        embedding = self.embedding_model.encode(chunk['text'])
                        embedding = embedding.tobytes()
                    except:
                        pass
                
                cursor.execute('''
                    INSERT INTO saju_knowledge_chunks 
                    (video_id, chunk_text, saju_terms, knowledge_category, confidence_score, embedding)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    video_id,
                    chunk['text'],
                    json.dumps(chunk['terms'], ensure_ascii=False),
                    chunk['category'],
                    0.8,  # ê¸°ë³¸ ì‹ ë¢°ë„
                    embedding
                ))
            
            # 3. ìš©ì–´ ë¹ˆë„ ì—…ë°ì´íŠ¸
            for category, terms in analysis['saju_terms_found'].items():
                for term_data in terms:
                    cursor.execute('''
                        INSERT OR IGNORE INTO saju_terminology (term, category, frequency)
                        VALUES (?, ?, 0)
                    ''', (term_data['term'], category))
                    
                    cursor.execute('''
                        UPDATE saju_terminology 
                        SET frequency = frequency + ?, updated_at = CURRENT_TIMESTAMP
                        WHERE term = ?
                    ''', (term_data['count'], term_data['term']))
            
            # 4. íŒ¨í„´ ì €ì¥
            for pattern in analysis['patterns']:
                cursor.execute('''
                    INSERT INTO learned_patterns 
                    (pattern_type, condition_terms, result_description, confidence)
                    VALUES (?, ?, ?, ?)
                ''', (
                    pattern['type'],
                    pattern['condition'],
                    pattern['result'],
                    pattern['confidence']
                ))
            
            conn.commit()
            print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {video_id}")
            
        except Exception as e:
            print(f"âŒ ì €ì¥ ì‹¤íŒ¨ ({video_id}): {str(e)}")
            conn.rollback()
        finally:
            conn.close()
    
    def process_video(self, video_id: str, video_info: Dict = None) -> bool:
        """
        ì˜ìƒ í•˜ë‚˜ì˜ ì „ì²´ ì²˜ë¦¬ ê³¼ì •
        
        Args:
            video_id: YouTube ì˜ìƒ ID
            video_info: ì˜ìƒ ë©”íƒ€ë°ì´í„° (ì„ íƒì‚¬í•­)
            
        Returns:
            ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€
        """
        try:
            print(f"ğŸ¥ ì˜ìƒ ì²˜ë¦¬ ì‹œì‘: {video_id}")
            
            # 1. ìë§‰ ì¶”ì¶œ
            transcript_data = self.extract_transcript(video_id)
            if not transcript_data:
                print(f"âŒ ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {video_id}")
                return False
            
            print(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ: {transcript_data['word_count']}ë‹¨ì–´, {transcript_data['language']}")
            
            # 2. ë‚´ìš© ë¶„ì„
            analysis = self.analyze_saju_content(transcript_data['text'])
            
            print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼:")
            print(f"  - ê´€ë ¨ì„± ì ìˆ˜: {analysis['relevance_score']:.2f}")
            print(f"  - ë°œê²¬ëœ ì‚¬ì£¼ ìš©ì–´: {len(analysis['saju_terms_found'])}ê°œ ì¹´í…Œê³ ë¦¬")
            print(f"  - ì§€ì‹ ë©ì–´ë¦¬: {len(analysis['knowledge_chunks'])}ê°œ")
            print(f"  - íŒ¨í„´: {len(analysis['patterns'])}ê°œ")
            
            # ê´€ë ¨ì„±ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if analysis['relevance_score'] < 0.1:
                print(f"âš ï¸ ê´€ë ¨ì„± ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ì•„ ì €ì¥í•˜ì§€ ì•ŠìŒ: {analysis['relevance_score']:.2f}")
                return False
            
            # 3. ê²°ê³¼ ì €ì¥
            if not video_info:
                video_info = {'title': f'Video {video_id}', 'channel_title': 'Unknown'}
            
            self.save_transcript_analysis(video_id, video_info, transcript_data, analysis)
            
            return True
            
        except Exception as e:
            print(f"âŒ ì˜ìƒ ì²˜ë¦¬ ì‹¤íŒ¨ ({video_id}): {str(e)}")
            return False
    
    def batch_process_videos(self, video_list: List[Dict], max_videos: int = 20) -> Dict:
        """
        ì—¬ëŸ¬ ì˜ìƒì„ ì¼ê´„ ì²˜ë¦¬
        
        Args:
            video_list: ì˜ìƒ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{'video_id': ..., 'title': ..., ...}, ...]
            max_videos: ìµœëŒ€ ì²˜ë¦¬í•  ì˜ìƒ ê°œìˆ˜
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """
        results = {
            'total_videos': min(len(video_list), max_videos),
            'success_count': 0,
            'failed_count': 0,
            'total_knowledge_chunks': 0,
            'avg_relevance_score': 0.0
        }
        
        processed_videos = video_list[:max_videos]
        
        print(f"ğŸš€ ì¼ê´„ ì²˜ë¦¬ ì‹œì‘: {len(processed_videos)}ê°œ ì˜ìƒ")
        
        relevance_scores = []
        
        for i, video_info in enumerate(processed_videos, 1):
            print(f"\n[{i}/{len(processed_videos)}] ì²˜ë¦¬ ì¤‘...")
            
            video_id = video_info.get('video_id')
            if not video_id:
                results['failed_count'] += 1
                continue
            
            success = self.process_video(video_id, video_info)
            
            if success:
                results['success_count'] += 1
                # ê´€ë ¨ì„± ì ìˆ˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ì ¸ì˜¤ê¸°
                conn = sqlite3.connect(self.knowledge_db_path)
                cursor = conn.cursor()
                cursor.execute('SELECT saju_relevance_score FROM video_transcripts WHERE video_id = ?', (video_id,))
                row = cursor.fetchone()
                if row:
                    relevance_scores.append(row[0])
                conn.close()
            else:
                results['failed_count'] += 1
        
        # í†µê³„ ê³„ì‚°
        if relevance_scores:
            results['avg_relevance_score'] = sum(relevance_scores) / len(relevance_scores)
        
        # ì´ ì§€ì‹ ë©ì–´ë¦¬ ê°œìˆ˜
        conn = sqlite3.connect(self.knowledge_db_path)
        cursor = conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM saju_knowledge_chunks')
        results['total_knowledge_chunks'] = cursor.fetchone()[0]
        conn.close()
        
        print(f"\nğŸ‰ ì¼ê´„ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"  - ì„±ê³µ: {results['success_count']}ê°œ")
        print(f"  - ì‹¤íŒ¨: {results['failed_count']}ê°œ") 
        print(f"  - í‰ê·  ê´€ë ¨ì„±: {results['avg_relevance_score']:.2f}")
        print(f"  - ì´ ì§€ì‹ ë©ì–´ë¦¬: {results['total_knowledge_chunks']}ê°œ")
        
        return results

def main():
    """í…ŒìŠ¤íŠ¸ ë° ì˜ˆì‹œ ì‹¤í–‰"""
    analyzer = SajuTranscriptAnalyzer()
    
    # YouTube í¬ë¡¤ëŸ¬ë¡œ ì‚¬ì£¼ ì˜ìƒ ì°¾ê¸°
    crawler = YouTubeSajuCrawler()
    print("ğŸ” ì‚¬ì£¼ ê´€ë ¨ ì˜ìƒ ê²€ìƒ‰ ì¤‘...")
    
    videos = crawler.crawl_saju_videos(max_per_keyword=3)  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì ì€ ìˆ˜
    
    if not videos:
        print("âŒ ê²€ìƒ‰ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… {len(videos)}ê°œ ì˜ìƒ ë°œê²¬")
    
    # ìë§‰ ë¶„ì„ ë° í•™ìŠµ
    results = analyzer.batch_process_videos(videos, max_videos=5)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“ˆ ìµœì¢… ê²°ê³¼:")
    print(f"  - ì²˜ë¦¬ëœ ì˜ìƒ: {results['success_count']}/{results['total_videos']}")
    print(f"  - í•™ìŠµëœ ì§€ì‹ ë©ì–´ë¦¬: {results['total_knowledge_chunks']}ê°œ")
    print(f"  - í‰ê·  ê´€ë ¨ì„± ì ìˆ˜: {results['avg_relevance_score']:.2f}")
    
    # ë°ì´í„°ë² ì´ìŠ¤ í˜„í™©
    conn = sqlite3.connect(analyzer.knowledge_db_path)
    cursor = conn.cursor()
    
    cursor.execute('SELECT COUNT(*) FROM saju_terminology WHERE frequency > 0')
    term_count = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM learned_patterns')
    pattern_count = cursor.fetchone()[0]
    
    print(f"  - í•™ìŠµëœ ì‚¬ì£¼ ìš©ì–´: {term_count}ê°œ")
    print(f"  - ì¶”ì¶œëœ íŒ¨í„´: {pattern_count}ê°œ")
    
    conn.close()

if __name__ == "__main__":
    main()